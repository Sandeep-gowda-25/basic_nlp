{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0c5ab4-4dce-4645-914d-ea9d17dc32ad",
   "metadata": {},
   "source": [
    "Noise removal(stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ce3acb-a5b4-4147-9ebc-98d8c8059ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027ae86e-7873-4216-9896-24d34c2adaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple text needs cleaned'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a simple text which needs to be cleaned\"\n",
    "from nltk.corpus import stopwords\n",
    "def noise_removal(text):\n",
    "    words = text.split()\n",
    "    noise = stopwords.words('english')\n",
    "    cleaned_words = [word for word in words if word.lower() not in noise]\n",
    "    return ' '.join(cleaned_words)\n",
    "noise_removal(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d554115-b437-4e8a-9e67-d9dd86cb217f",
   "metadata": {},
   "source": [
    "regex cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2a964c-df04-473c-baa2-5e21f83fd813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a  simple text which  needs to be cleaned, '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"This is a 4 simple text which https://dummy.com needs to be \n",
    "cleaned, #test\"\"\"\n",
    "import re\n",
    "url_pattern = r'http[s]?://\\S+|www\\.\\S+' # to remove http:// , https://, www.\n",
    "numbers_pattern = r'\\d+' # to clean numbers\n",
    "hashtag_pattern = r'#\\S+'\n",
    "newline_pattern = r'[\\r\\n]+'\n",
    "\n",
    "def regex_cleaning(text):\n",
    "    url_cleaned = re.sub(url_pattern,'',text)\n",
    "    numbers_cleaned = re.sub(numbers_pattern,'',url_cleaned)\n",
    "    hashtag_cleaned = re.sub(hashtag_pattern,'',numbers_cleaned)\n",
    "    newline_cleaned = re.sub(newline_pattern,'',hashtag_cleaned)\n",
    "    return newline_cleaned\n",
    "regex_cleaning(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba55c8f-bb97-4343-a2fc-4475975f69c8",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a57af8f-03ca-46e1-ba22-549c96b59b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'song'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"songs\"\n",
    "lemmatizer.lemmatize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83042850-ab57-45d0-9651-7ae64c431de7",
   "metadata": {},
   "source": [
    "custom word standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a606f1-d31f-421b-bc30-f4c4e2089423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi good morning I sent you an attachment in whatsapp post that on X and retweet aswell.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_abvs = {'wa':'whatsapp','gm': 'good morning', 'rt':'retweet','awsm':'awsome'}\n",
    "text='Hi gm I sent you an attachment in wa post that on X and rt aswell.'\n",
    "words=text.split()\n",
    "new_words=[custom_abvs[word] if word in custom_abvs.keys() else word for word in words]\n",
    "' '.join(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390fb84-8c7a-4051-9d50-b097892a5690",
   "metadata": {},
   "source": [
    "part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03f9ede-eaad-4a3e-974b-67dfcebab3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4b0c97-1547-4af9-b3ad-a7327249f77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('trying', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('recall', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('NLP', 'NNP'),\n",
       " ('activites', 'VBZ'),\n",
       " ('here', 'RB')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "text = 'I am trying to recall my NLP activites here'\n",
    "words = text.split()\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea386d6-bf7a-4dc1-829b-e04f7dde871e",
   "metadata": {},
   "source": [
    "Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c560a12-ab29-4933-9392-a9e86361454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scar.v.01\n",
      "score.v.03\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "s1 = 'Please mark my attendance'\n",
    "s2 = 'I got good marks'\n",
    "print(lesk(s1.split(),'mark').name())##mark of ==>verb\n",
    "print(lesk(s2.split(),'mark').name())## mark of ==>noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd200b-dd34-466d-88c4-32f0e2a6fea0",
   "metadata": {},
   "source": [
    "Named Entiy extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0200221-f997-4680-a979-27a6e90bebc1",
   "metadata": {},
   "source": [
    "noun extraction using pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2372916-73e8-466d-ad10-d6d72c9da86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sergey', 'NNP'),\n",
       " ('Brin,', 'NNP'),\n",
       " ('the', 'DT'),\n",
       " ('manager', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Google', 'NNP'),\n",
       " ('Inc.', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('walking', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('streets', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Sergey Brin, the manager of Google Inc. is walking in the streets of New York\"\n",
    "pos_tag(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7321c0-469f-4795-a7c4-88e3a3c3291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_items =  pos_tag(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a090b6d-61a7-4201-be33-95e95cacd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [item[0] for item in pos_items if 'NN' in item[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08f0dae-f35c-4680-a818-6d7166dde158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sergey', 'Brin,', 'manager', 'Google', 'Inc.', 'streets', 'New', 'York']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dd7a793-1216-44ba-9efb-da9a8005b1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('street.n.03')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk(text.split(),'streets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384e57e-faa2-460c-867d-1a7780c69bc5",
   "metadata": {},
   "source": [
    "Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c07b53-313a-4b84-9eb6-2f6efa64e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef7ea5d-1c00-41d2-9514-e85de58cbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq corpora Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ec695a-cc82-4ffc-bd68-ab5d8cf05dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.083*\"to\" + 0.058*\"My\" + 0.058*\"sister\" + 0.058*\"my\" + 0.033*\"sugar,\" + 0.033*\"not\" + 0.033*\"Sugar\" + 0.033*\"is\" + 0.033*\"bad\" + 0.033*\"consume.\"'), (1, '0.029*\"driving\" + 0.029*\"dance\" + 0.029*\"time\" + 0.029*\"father\" + 0.029*\"around\" + 0.029*\"a\" + 0.029*\"practice.\" + 0.029*\"spends\" + 0.029*\"of\" + 0.029*\"lot\"'), (2, '0.060*\"driving\" + 0.060*\"cause\" + 0.060*\"Doctors\" + 0.060*\"and\" + 0.060*\"that\" + 0.060*\"blood\" + 0.060*\"increased\" + 0.060*\"stress\" + 0.060*\"pressure.\" + 0.060*\"may\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.Â  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "# print(dictionary)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "# print(doc_term_matrix)\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8a12338-0e06-4abb-a975-cc2c6fa93830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.083*\"to\" + 0.058*\"my\" + 0.058*\"My\" + 0.058*\"sister\" + 0.033*\"not\"'),\n",
       " (1,\n",
       "  '0.029*\"driving\" + 0.029*\"a\" + 0.029*\"practice.\" + 0.029*\"dance\" + 0.029*\"around\"'),\n",
       " (2,\n",
       "  '0.060*\"driving\" + 0.060*\"cause\" + 0.060*\"Doctors\" + 0.060*\"and\" + 0.060*\"that\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd74236-1adb-4dd1-9987-2f04f9654251",
   "metadata": {},
   "source": [
    "Tf-Idf(term frequency and inverse document frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9d0a2-6a0f-4b3e-bb91-f5add81759d3",
   "metadata": {},
   "source": [
    "tf - [count of a term in a document]\n",
    "idf - [log of ratio of total documents to documents having term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2438b21c-e8b1-47f7-9d53-158fce8b564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a4bc5a0-d963-4bcc-96dd-bac406bd9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d228a887-61be-49c2-b28c-15506a447ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.5844829010200651\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 1)\t0.34520501686496574\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179902c1-02aa-483f-aa4e-a090ad1446e3",
   "metadata": {},
   "source": [
    "text classification using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c78052-c9bc-4eef-b79d-b56c6857fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    {\"text\": \"I love this product! It's amazing!\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"This is the worst experience I've ever had.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The movie was fantastic, a must-watch!\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"I don't think I will buy this again, very disappointing.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The food was good, but the service was slow.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"Excellent customer support, I will definitely recommend it!\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"Worst purchase I've made in a while, very bad quality.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"Absolutely fantastic! I can't believe how good it was.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"I hated every second of this book, it was so boring.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"Highly recommend! Worth every penny.\", \"label\": \"Positive\"}\n",
    "]\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.DataFrame(sample_data)\n",
    "X_train,X_test, y_train,y_test = train_test_split(df[\"text\"],df[\"label\"],test_size=0.2,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c712ab65-26ee-4da7-ae4d-6a6c8e8f99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "944abe21-6e61-43d7-a0d2-6b822390b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d822608-3d77-47d4-94be-76624d13e7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative', 'Positive'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(train_vectors,y_train)\n",
    "model.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37ced8b7-ee43-4b83-b023-3d105ba9dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    I don't think I will buy this again, very disa...\n",
       "5    Excellent customer support, I will definitely ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e88c686-e5b5-4f4e-8e1c-38e38676c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce7a591b-4db0-4d1b-baef-1ac7704b1c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         1\n",
      "    Positive       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,model.predict(test_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c0b06-ff5f-4ee7-bcfd-2d6992f903b4",
   "metadata": {},
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adadc9d8-f908-4421-8801-fdd6e9722c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.6172134]\n",
      " [0.6172134 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "#bag of words approach\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentences = [\"This is a sample sentence to test\",\n",
    "            \"To test the similarity using this sample\"]\n",
    "\n",
    "vectors = CountVectorizer().fit_transform(sentences)\n",
    "print(cosine_similarity(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed96ae9f-0556-4362-af6a-74d77c66ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.45026814]\n",
      " [0.45026814 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#tf-idf approach\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sentences = [\"This is a sample sentence to test\",\n",
    "            \"To test the similarity using this sample\"]\n",
    "vectors = TfidfVectorizer().fit_transform(sentences)\n",
    "print(cosine_similarity(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dd6b65b-f765-49a1-8bde-b11cf0ae5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "#jaccard way\n",
    "sentence1 = \"This is a sample sentence to test\"\n",
    "sentence2 = \"To test the similarity using this sample\"\n",
    "set1 = set(sentence1.lower().split())\n",
    "set2 = set(sentence2.lower().split())\n",
    "similarity = len(set1.intersection(set2))/len(set1.union(set2))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12b32449-dd98-42e4-9540-ef46e91e7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36109a30-54b3-4939-8b42-83e384651267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lev_distance is 31\n",
      "0.22499999999999998\n"
     ]
    }
   ],
   "source": [
    "#levenshtein distance approach\n",
    "import Levenshtein\n",
    "sentence1 = \"This is a sample sentence to test\"\n",
    "sentence2 = \"To test the similarity using this sample\"\n",
    "distance = Levenshtein.distance(sentence1,sentence2)\n",
    "print(f\"lev_distance is {distance}\")\n",
    "similarity = 1 - (distance/max(len(sentence1),len(sentence2)))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3874635a-7e56-49ad-943d-7008fa179a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22499999999999998\n"
     ]
    }
   ],
   "source": [
    "#word2vec approach\n",
    "from gensim.downloader import load\n",
    "word_vectors = load('glove-wiki-gigaword-100')\n",
    "sentence1 = \"This is a sample sentence to test\"\n",
    "sentence2 = \"To test the similarity using this sample\"\n",
    "def get_vectors(sentence):\n",
    "    word_vecs = [word_vectors[word] for word in sentence.split() if word in word_vectors]\n",
    "    import numpy as np\n",
    "    if not word_vecs:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(word_vecs,axis=0)\n",
    "vec1 = get_vectors(sentence1)\n",
    "vec2 = get_vectors(sentence2)\n",
    "similairty = cosine_similarity([vec1],[vec2])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a246244-eb6a-40ce-9fb3-b08a8efde39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
